
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>wbia_cnn.custom_layers &#8212; wbia-cnn 3.3.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for wbia_cnn.custom_layers</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">tensor</span> <span class="k">as</span> <span class="n">T</span>  <span class="c1"># NOQA</span>
<span class="kn">import</span> <span class="nn">Lasagne</span> <span class="k">as</span> <span class="nn">lasagne</span>
<span class="kn">from</span> <span class="nn">Lasagne.lasagne</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">nonlinearities</span>
<span class="kn">from</span> <span class="nn">wbia_cnn</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">import</span> <span class="nn">utool</span> <span class="k">as</span> <span class="nn">ut</span>

<span class="p">(</span><span class="nb">print</span><span class="p">,</span> <span class="n">rrr</span><span class="p">,</span> <span class="n">profile</span><span class="p">)</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">inject2</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="n">FORCE_CPU</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># ut.get_argflag(&#39;--force-cpu&#39;)</span>
<span class="n">USING_GPU</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">try</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">FORCE_CPU</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s1">&#39;GPU is forced off&#39;</span><span class="p">)</span>
    <span class="c1"># use cuda_convnet for a speed improvement</span>
    <span class="c1"># will not be available without a GPU</span>

    <span class="n">conv_impl</span> <span class="o">=</span> <span class="s1">&#39;cuDNN&#39;</span>
    <span class="c1"># conv_impl = &#39;cuda_convnet&#39;</span>
    <span class="k">if</span> <span class="n">ut</span><span class="o">.</span><span class="n">get_computer_name</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;hyrule&#39;</span><span class="p">:</span>
        <span class="c1"># cuda_convnet seems broken on hyrule</span>
        <span class="n">conv_impl</span> <span class="o">=</span> <span class="s1">&#39;cuDNN&#39;</span>

    <span class="c1"># http://lasagne.readthedocs.org/en/latest/modules/layers/conv.html#layers.Conv2DLayer</span>

    <span class="k">if</span> <span class="n">conv_impl</span> <span class="o">==</span> <span class="s1">&#39;cuda_convnet&#39;</span><span class="p">:</span>
        <span class="c1"># cannot handle non-square images (pylearn2 module)</span>
        <span class="kn">import</span> <span class="nn">layers.cuda_convnet</span>

        <span class="n">Conv2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">cuda_convnet</span><span class="o">.</span><span class="n">Conv2DCCLayer</span>
        <span class="n">MaxPool2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">cuda_convnet</span><span class="o">.</span><span class="n">MaxPool2DCCLayer</span>
    <span class="k">elif</span> <span class="n">conv_impl</span> <span class="o">==</span> <span class="s1">&#39;cuDNN&#39;</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">layers.dnn</span>

        <span class="n">Conv2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">dnn</span><span class="o">.</span><span class="n">Conv2DDNNLayer</span>
        <span class="n">MaxPool2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">dnn</span><span class="o">.</span><span class="n">MaxPool2DDNNLayer</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Need cuda convnet for background model otherwise</span>
<span class="sd">        &lt;type &#39;exceptions.ValueError&#39;&gt;: GpuReshape: cannot reshape input of shape (128, 12, 26, 26) to shape (128, 676).</span>
<span class="sd">        Apply node that caused the error: GpuReshape{2}(GpuElemwise{Composite{((i0 * (i1 + i2)) + (i3 * Abs((i1 + i2))))}}[(0, 1)].0, TensorConstant{[128 676]})</span>
<span class="sd">        Toposort index: 36</span>
<span class="sd">        Inputs types: [CudaNdarrayType(float32, 4D), TensorType(int64, vector)]</span>
<span class="sd">        Inputs shapes: [(128, 12, 26, 26), (2,)]</span>
<span class="sd">        Inputs strides: [(676, 86528, 26, 1), (8,)]</span>
<span class="sd">        Inputs values: [&#39;not shown&#39;, array([128, 676])]</span>
<span class="sd">        Outputs clients: [[GpuDot22(GpuReshape{2}.0, GpuReshape{2}.0)]]</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="k">elif</span> <span class="n">conv_impl</span> <span class="o">==</span> <span class="s1">&#39;gemm&#39;</span><span class="p">:</span>
        <span class="c1"># Dont use gemm</span>
        <span class="kn">import</span> <span class="nn">layers.corrmm</span>

        <span class="n">Conv2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">corrmm</span><span class="o">.</span><span class="n">Conv2DLayer</span>
        <span class="n">MaxPool2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">corrmm</span><span class="o">.</span><span class="n">Conv2DLayer</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;conv_impl = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">conv_impl</span><span class="p">,))</span>

    <span class="n">USING_GPU</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="p">(</span><span class="ne">Exception</span><span class="p">,</span> <span class="ne">ImportError</span><span class="p">)</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
    <span class="n">Conv2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Conv2DLayer</span>
    <span class="n">MaxPool2DLayer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2DLayer</span>

    <span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">VERBOSE_CNN</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Conv2DLayer = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">Conv2DLayer</span><span class="p">,))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MaxPool2DLayer = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">MaxPool2DLayer</span><span class="p">,))</span>

    <span class="k">if</span> <span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
        <span class="n">ut</span><span class="o">.</span><span class="n">printex</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="s1">&#39;WARNING: GPU seems unavailable&#39;</span><span class="p">,</span> <span class="n">iswarning</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">VERBOSE_CNN</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s1">&#39;lasagne.__version__ = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">lasagne</span><span class="p">,</span> <span class="s1">&#39;__version__&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lasagne.__file__ = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">lasagne</span><span class="p">,</span> <span class="s1">&#39;__file__&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;theano.__version__ = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">theano</span><span class="p">,</span> <span class="s1">&#39;__version__&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;theano.__file__ = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">theano</span><span class="p">,</span> <span class="s1">&#39;__file__&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),))</span>


<div class="viewcode-block" id="L1NormalizeLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L1NormalizeLayer">[docs]</a><span class="k">class</span> <span class="nc">L1NormalizeLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1NormalizeLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="L1NormalizeLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L1NormalizeLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">ell1_norm</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">abs_</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output_</span> <span class="o">=</span> <span class="n">input_</span> <span class="o">/</span> <span class="n">ell1_norm</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">output_</span></div></div>


<div class="viewcode-block" id="LocallyConnected2DLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.LocallyConnected2DLayer">[docs]</a><span class="nd">@six</span><span class="o">.</span><span class="n">add_metaclass</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">ReloadingMetaclass</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LocallyConnected2DLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Copy of the Conv2D layer that needs to be adapted into a locally connected layer</span>

<span class="sd">    Args:</span>
<span class="sd">        incoming (layers.Layer):</span>
<span class="sd">        num_filters (?):</span>
<span class="sd">        filter_size (?):</span>
<span class="sd">        stride (tuple): (default = (1, 1))</span>
<span class="sd">        pad (int): (default = 0)</span>
<span class="sd">        untie_biases (bool): (default = False)</span>
<span class="sd">        W (GlorotUniform): (default = &lt;init.GlorotUniform object at 0x7f551a3537d0&gt;)</span>
<span class="sd">        b (Constant): (default = &lt;init.Constant object at 0x7f551a33ecd0&gt;)</span>
<span class="sd">        nonlinearity (function): (default = &lt;function rectify at 0x7f55307989b0&gt;)</span>
<span class="sd">        convolution (function): (default = &lt;function conv2d at 0x7f55330148c0&gt;)</span>

<span class="sd">    CommandLine:</span>
<span class="sd">        python -m wbia_cnn.custom_layers --exec-__init__</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # DISABLE_DOCTEST</span>
<span class="sd">        &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">        &gt;&gt;&gt; incoming = testdata_input_layer(item_shape=(3,8,8), batch_size=4)</span>
<span class="sd">        &gt;&gt;&gt; num_filters = 64</span>
<span class="sd">        &gt;&gt;&gt; filter_size = (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; stride = (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; pad = 0</span>
<span class="sd">        &gt;&gt;&gt; untie_biases = False</span>
<span class="sd">        &gt;&gt;&gt; W = init.GlorotUniform()</span>
<span class="sd">        &gt;&gt;&gt; b = init.Constant(0.)</span>
<span class="sd">        &gt;&gt;&gt; nonlinearity = nonlinearities.rectify</span>
<span class="sd">        &gt;&gt;&gt; convolution = T.nnet.conv2d</span>
<span class="sd">        &gt;&gt;&gt; self = LocallyConnected2DLayer(incoming, num_filters, filter_size,</span>
<span class="sd">        &gt;&gt;&gt;                                stride, pad, untie_biases, W, b,</span>
<span class="sd">        &gt;&gt;&gt;                                nonlinearity, convolution)</span>

<span class="sd">    Ignore:</span>
<span class="sd">        self.get_output_rc(self.input_shape)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">incoming</span><span class="p">,</span>
        <span class="n">num_filters</span><span class="p">,</span>
        <span class="n">filter_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">untie_biases</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">W</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">GlorotUniform</span><span class="p">(),</span>
        <span class="n">b</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">rectify</span><span class="p">,</span>
        <span class="n">convolution</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LocallyConnected2DLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">incoming</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nonlinearity</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearities</span><span class="o">.</span><span class="n">identity</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span> <span class="o">=</span> <span class="n">num_filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">as_tuple</span><span class="p">(</span><span class="n">filter_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">as_tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">untie_biases</span> <span class="o">=</span> <span class="n">untie_biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span> <span class="o">=</span> <span class="n">convolution</span>

        <span class="k">if</span> <span class="n">pad</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;`same` padding requires odd filter size.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad</span> <span class="o">==</span> <span class="s1">&#39;valid&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">pad</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">as_tuple</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_param</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_W_shape</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">untie_biases</span><span class="p">:</span>
                <span class="n">biases_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">biases_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_filters</span><span class="p">,)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_param</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">biases_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">regularizable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<div class="viewcode-block" id="LocallyConnected2DLayer.get_W_shape"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.LocallyConnected2DLayer.get_W_shape">[docs]</a>    <span class="k">def</span> <span class="nf">get_W_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get the shape of the weight matrix `W`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tuple of int</span>
<span class="sd">            The shape of the weight matrix.</span>

<span class="sd">            (should have a different conv matrix for each output node)</span>
<span class="sd">            (ie NO WEIGHT SHARING)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">output_rows</span><span class="p">,</span> <span class="n">output_cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_rc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span>
            <span class="n">num_input_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">output_rows</span><span class="p">,</span>
            <span class="n">output_cols</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="LocallyConnected2DLayer.get_output_rc"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.LocallyConnected2DLayer.get_output_rc">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_rc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="n">output_rows</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">conv_output_length</span><span class="p">(</span>
            <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">output_columns</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">conv_output_length</span><span class="p">(</span>
            <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output_rows</span><span class="p">,</span> <span class="n">output_columns</span></div>

<div class="viewcode-block" id="LocallyConnected2DLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.LocallyConnected2DLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">output_rows</span><span class="p">,</span> <span class="n">output_columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_rc</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">output_rows</span><span class="p">,</span> <span class="n">output_columns</span><span class="p">)</span></div>

<div class="viewcode-block" id="LocallyConnected2DLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.LocallyConnected2DLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># The optional input_shape argument is for when get_output_for is</span>
        <span class="c1"># called directly with a different shape than self.input_shape.</span>
        <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span><span class="p">:</span>
            <span class="c1"># simulate same convolution by cropping a full convolution</span>
            <span class="n">conved</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                <span class="n">subsample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">image_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
                <span class="n">filter_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_W_shape</span><span class="p">(),</span>
                <span class="n">border_mode</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">crop_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">crop_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">conved</span> <span class="o">=</span> <span class="n">conved</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">crop_x</span> <span class="p">:</span> <span class="o">-</span><span class="n">crop_x</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span> <span class="n">crop_y</span> <span class="p">:</span> <span class="o">-</span><span class="n">crop_y</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># no padding needed, or explicit padding of input needed</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">==</span> <span class="s1">&#39;full&#39;</span><span class="p">:</span>
                <span class="n">border_mode</span> <span class="o">=</span> <span class="s1">&#39;full&#39;</span>
                <span class="n">pad</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span><span class="p">:</span>
                <span class="n">border_mode</span> <span class="o">=</span> <span class="s1">&#39;valid&#39;</span>
                <span class="n">pad</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">border_mode</span> <span class="o">=</span> <span class="s1">&#39;valid&#39;</span>
                <span class="n">pad</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
            <span class="k">if</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]:</span>
                <span class="nb">input</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">theano_extensions</span><span class="o">.</span><span class="n">padding</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">batch_ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="kc">None</span>
                    <span class="k">if</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                    <span class="kc">None</span>
                    <span class="k">if</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                <span class="p">)</span>
            <span class="n">conved</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                <span class="n">subsample</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">image_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
                <span class="n">filter_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_W_shape</span><span class="p">(),</span>
                <span class="n">border_mode</span><span class="o">=</span><span class="n">border_mode</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">conved</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">untie_biases</span><span class="p">:</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">conved</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">activation</span> <span class="o">=</span> <span class="n">conved</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span></div></div>


<span class="c1"># @six.add_metaclass(ut.ReloadingMetaclass)</span>
<div class="viewcode-block" id="L2NormalizeLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L2NormalizeLayer">[docs]</a><span class="k">class</span> <span class="nc">L2NormalizeLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Normalizes the outputs of a layer to have an L2 norm of 1.</span>
<span class="sd">    This is useful for siamese networks who&#39;s outputs will be comparsed using</span>
<span class="sd">    the L2 distance.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L2NormalizeLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>

<div class="viewcode-block" id="L2NormalizeLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L2NormalizeLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        CommandLine:</span>
<span class="sd">            python -m wbia_cnn.custom_layers --test-L2NormalizeLayer.get_output_for</span>

<span class="sd">        Example0:</span>
<span class="sd">            &gt;&gt;&gt; # ENABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; # l2 normalization on batches of vector encodings</span>
<span class="sd">            &gt;&gt;&gt; input_layer = testdata_input_layer(item_shape=(8,), batch_size=4)</span>
<span class="sd">            &gt;&gt;&gt; inputdata_ = np.random.rand(*input_layer.shape).astype(np.float32)</span>
<span class="sd">            &gt;&gt;&gt; axis = 1</span>
<span class="sd">            &gt;&gt;&gt; self = L2NormalizeLayer(input_layer, axis=axis)</span>
<span class="sd">            &gt;&gt;&gt; # Test numpy version</span>
<span class="sd">            &gt;&gt;&gt; T = np</span>
<span class="sd">            &gt;&gt;&gt; input_ = inputdata_</span>
<span class="sd">            &gt;&gt;&gt; output_np = self.get_output_for(inputdata_, T=np)</span>
<span class="sd">            &gt;&gt;&gt; assert np.all(np.isclose(np.linalg.norm(output_np, axis=axis), 1.0))</span>
<span class="sd">            &gt;&gt;&gt; # Test theano version</span>
<span class="sd">            &gt;&gt;&gt; T = theano.tensor</span>
<span class="sd">            &gt;&gt;&gt; input_expr = input_ = T.matrix(name=&#39;vector_input&#39;)</span>
<span class="sd">            &gt;&gt;&gt; output_expr = self.get_output_for(input_expr, T=T)</span>
<span class="sd">            &gt;&gt;&gt; output_T = output_expr.eval({input_expr: inputdata_})</span>
<span class="sd">            &gt;&gt;&gt; print(output_T)</span>
<span class="sd">            &gt;&gt;&gt; assert np.all(np.isclose(output_T, output_np)), &#39;theano and numpy diagree&#39;</span>

<span class="sd">        Example1:</span>
<span class="sd">            &gt;&gt;&gt; # ENABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; # l2 normalization on batches of image filters</span>
<span class="sd">            &gt;&gt;&gt; input_layer = testdata_input_layer(item_shape=(3, 2, 2), batch_size=4)</span>
<span class="sd">            &gt;&gt;&gt; inputdata_ = np.random.rand(*input_layer.shape).astype(np.float32)</span>
<span class="sd">            &gt;&gt;&gt; axis = 2</span>
<span class="sd">            &gt;&gt;&gt; self = L2NormalizeLayer(input_layer, axis=axis)</span>
<span class="sd">            &gt;&gt;&gt; # Test numpy version</span>
<span class="sd">            &gt;&gt;&gt; T = np</span>
<span class="sd">            &gt;&gt;&gt; input_ = inputdata_</span>
<span class="sd">            &gt;&gt;&gt; output_np = self.get_output_for(inputdata_, T=np)</span>
<span class="sd">            &gt;&gt;&gt; output_flat_np = output_np.reshape(np.prod(input_layer.shape[0:2]), np.prod(input_layer.shape[2:4]))</span>
<span class="sd">            &gt;&gt;&gt; assert np.all(np.isclose(np.linalg.norm(output_flat_np, axis=1), 1.0))</span>
<span class="sd">            &gt;&gt;&gt; # Test theano version</span>
<span class="sd">            &gt;&gt;&gt; T = theano.tensor</span>
<span class="sd">            &gt;&gt;&gt; input_expr = input_ = T.tensor4(name=&#39;image_filter_input&#39;)</span>
<span class="sd">            &gt;&gt;&gt; output_expr = self.get_output_for(input_expr, T=T)</span>
<span class="sd">            &gt;&gt;&gt; output_T = output_expr.eval({input_expr: inputdata_})</span>
<span class="sd">            &gt;&gt;&gt; print(output_T)</span>
<span class="sd">            &gt;&gt;&gt; assert np.all(np.isclose(output_T, output_np)), &#39;theano and numpy diagree&#39;</span>
<span class="sd">            &gt;&gt;&gt; #output_T = utils.evaluate_symbolic_layer(self.get_output_for, inputdata_, T.tensor4, T=theano.tensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span>

        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span>
        <span class="n">rest_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">:]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">rest_size</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">rest_shape</span><span class="p">)</span>

        <span class="c1"># reshape to two dimensions</span>
        <span class="n">input_reshaped_</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">rest_size</span><span class="p">))</span>
        <span class="c1"># if T is np:</span>
        <span class="c1">#    #input_reshaped_ = input_.reshape(batch_shape + (rest_size,))</span>
        <span class="c1"># else:</span>
        <span class="c1">#    # hack because I don&#39;t know how to get ndim yet</span>
        <span class="c1">#    if axis == 1:</span>
        <span class="c1">#        input_reshaped_ = input_.reshape(batch_shape + (rest_size,), ndim=2)</span>
        <span class="c1">#    elif axis == 2:</span>
        <span class="c1">#        input_reshaped_ = input_.reshape(batch_shape + (rest_size,), ndim=3)</span>

        <span class="n">ell2_norm</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">input_reshaped_</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">T</span> <span class="ow">is</span> <span class="n">np</span><span class="p">:</span>
            <span class="c1"># outputreshaped_ = input_reshaped_ / ell2_norm[..., None]</span>
            <span class="n">outputreshaped_</span> <span class="o">=</span> <span class="n">input_reshaped_</span> <span class="o">/</span> <span class="n">ell2_norm</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">output_</span> <span class="o">=</span> <span class="n">outputreshaped_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outputreshaped_</span> <span class="o">=</span> <span class="n">input_reshaped_</span> <span class="o">/</span> <span class="n">ell2_norm</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">output_</span> <span class="o">=</span> <span class="n">outputreshaped_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            <span class="n">output_</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;l2normalized(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="c1"># .dimshuffle(0, &#39;x&#39;, 1)</span>
        <span class="k">return</span> <span class="n">output_</span></div></div>


<div class="viewcode-block" id="L2SquaredDistanceLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L2SquaredDistanceLayer">[docs]</a><span class="k">class</span> <span class="nc">L2SquaredDistanceLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L2SquaredDistanceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="L2SquaredDistanceLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L2SquaredDistanceLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,)</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span></div>

<div class="viewcode-block" id="L2SquaredDistanceLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L2SquaredDistanceLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Split batch into pairs</span>
        <span class="n">G1</span><span class="p">,</span> <span class="n">G2</span> <span class="o">=</span> <span class="n">input_</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">input_</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">E</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">power</span><span class="p">((</span><span class="n">G1</span> <span class="o">-</span> <span class="n">G2</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">E</span></div></div>


<div class="viewcode-block" id="L1DistanceLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L1DistanceLayer">[docs]</a><span class="k">class</span> <span class="nc">L1DistanceLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1DistanceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="L1DistanceLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L1DistanceLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,)</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span></div>

<div class="viewcode-block" id="L1DistanceLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.L1DistanceLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Split batch into pairs</span>
        <span class="n">G1</span><span class="p">,</span> <span class="n">G2</span> <span class="o">=</span> <span class="n">input_</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">input_</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">E</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">abs_</span><span class="p">((</span><span class="n">G1</span> <span class="o">-</span> <span class="n">G2</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">E</span></div></div>


<div class="viewcode-block" id="testdata_input_layer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.testdata_input_layer">[docs]</a><span class="k">def</span> <span class="nf">testdata_input_layer</span><span class="p">(</span><span class="n">item_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">item_shape</span>
    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_layer</span></div>


<div class="viewcode-block" id="SiameseConcatLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.SiameseConcatLayer">[docs]</a><span class="k">class</span> <span class="nc">SiameseConcatLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    TODO checkout layers.merge.ConcatLayer</span>

<span class="sd">    Takes two network representations in the batch and combines them along an axis.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="n">data_per_label</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SiameseConcatLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_per_label</span> <span class="o">=</span> <span class="n">data_per_label</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>

<div class="viewcode-block" id="SiameseConcatLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.SiameseConcatLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape: shape being fed into this layer</span>
<span class="sd">            axis: overrideable for tests</span>

<span class="sd">        CommandLine:</span>
<span class="sd">            python -m wbia_cnn.custom_layers --test-get_output_shape_for</span>

<span class="sd">        Example0:</span>
<span class="sd">            &gt;&gt;&gt; # ENABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; input_layer = testdata_input_layer(item_shape=(3, 8, 16))</span>
<span class="sd">            &gt;&gt;&gt; self = SiameseConcatLayer(input_layer)</span>
<span class="sd">            &gt;&gt;&gt; input_shape = input_layer.shape</span>
<span class="sd">            &gt;&gt;&gt; output_shape_list = [self.get_output_shape_for(input_shape, axis) for axis in [1, 2, 3, -3, -2, -1]]</span>
<span class="sd">            &gt;&gt;&gt; result = str(output_shape_list[0:3]) + &#39;\n&#39; + str(output_shape_list[3:])</span>
<span class="sd">            &gt;&gt;&gt; print(result)</span>
<span class="sd">            [(64, 6, 8, 16), (64, 3, 16, 16), (64, 3, 8, 32)]</span>
<span class="sd">            [(64, 6, 8, 16), (64, 3, 16, 16), (64, 3, 8, 32)]</span>

<span class="sd">        Example1:</span>
<span class="sd">            &gt;&gt;&gt; # ENABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; input_layer = testdata_input_layer(item_shape=(1024,))</span>
<span class="sd">            &gt;&gt;&gt; self = SiameseConcatLayer(input_layer)</span>
<span class="sd">            &gt;&gt;&gt; input_shape = input_layer.shape</span>
<span class="sd">            &gt;&gt;&gt; output_shape_list = [self.get_output_shape_for(input_shape, axis) for axis in [1, -1]]</span>
<span class="sd">            &gt;&gt;&gt; result = output_shape_list</span>
<span class="sd">            &gt;&gt;&gt; print(result)</span>
<span class="sd">            [(64, 2048), (64, 2048)]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># allow override for tests</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;self.axis=</span><span class="si">%r</span><span class="s1"> cannot be 0&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,)</span>
        <span class="n">new_batch_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_per_label</span><span class="p">,)</span>
        <span class="n">new_shape_middle</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_per_label</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">shape_front</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span>
            <span class="n">shape_end</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape_front</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span>
            <span class="n">shape_end</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="n">new_batch_shape</span> <span class="o">+</span> <span class="n">shape_front</span> <span class="o">+</span> <span class="n">new_shape_middle</span> <span class="o">+</span> <span class="n">shape_end</span>
        <span class="k">return</span> <span class="n">output_shape</span></div>

<div class="viewcode-block" id="SiameseConcatLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.SiameseConcatLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        CommandLine:</span>
<span class="sd">            python -m wbia_cnn.custom_layers --test-SiameseConcatLayer.get_output_for:1 --show</span>

<span class="sd">        Example0:</span>
<span class="sd">            &gt;&gt;&gt; # ENABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; input_shape = (128, 1024)</span>
<span class="sd">            &gt;&gt;&gt; input_layer = layers.InputLayer(shape=input_shape)</span>
<span class="sd">            &gt;&gt;&gt; self = SiameseConcatLayer(input_layer)</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">            &gt;&gt;&gt; input_ = np.random.rand(*input_shape)</span>
<span class="sd">            &gt;&gt;&gt; T = np</span>
<span class="sd">            &gt;&gt;&gt; output_ = self.get_output_for(input_, T=T)</span>
<span class="sd">            &gt;&gt;&gt; target_shape = self.get_output_shape_for(input_shape)</span>
<span class="sd">            &gt;&gt;&gt; result = output_.shape</span>
<span class="sd">            &gt;&gt;&gt; print(result)</span>
<span class="sd">            &gt;&gt;&gt; assert target_shape == result</span>
<span class="sd">            (64, 2048)</span>

<span class="sd">        Example1:</span>
<span class="sd">            &gt;&gt;&gt; # DISABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn import utils</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn import draw_net</span>
<span class="sd">            &gt;&gt;&gt; import theano</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; input_layer = layers.InputLayer(shape=(4, 3, 32, 32))</span>
<span class="sd">            &gt;&gt;&gt; cs_layer = CenterSurroundLayer(input_layer)</span>
<span class="sd">            &gt;&gt;&gt; # Make sure that this can concat center surround properly</span>
<span class="sd">            &gt;&gt;&gt; self = SiameseConcatLayer(cs_layer, axis=2, data_per_label=4)</span>
<span class="sd">            &gt;&gt;&gt; data = utils.testdata_imglist()[0]</span>
<span class="sd">            &gt;&gt;&gt; inputdata_ = utils.convert_cv2_images_to_theano_images(data)</span>
<span class="sd">            &gt;&gt;&gt; outputdata_ = cs_layer.get_output_for(inputdata_, T=np)</span>
<span class="sd">            &gt;&gt;&gt; input_ = outputdata_</span>
<span class="sd">            &gt;&gt;&gt; output_ = self.get_output_for(input_, T=np)</span>
<span class="sd">            &gt;&gt;&gt; ut.quit_if_noshow()</span>
<span class="sd">            &gt;&gt;&gt; img_list = utils.convert_theano_images_to_cv2_images(output_)</span>
<span class="sd">            &gt;&gt;&gt; interact_image_list(img_list, num_per_page=2)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_per_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_per_label</span>
        <span class="n">split_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_</span><span class="p">[</span><span class="n">count</span><span class="p">::</span><span class="n">data_per_label</span><span class="p">]</span> <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data_per_label</span><span class="p">)]</span>
        <span class="n">output_</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">split_inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="c1"># input1, input2 = input_[0::2], input_[1::2]</span>
        <span class="c1"># output_ =  T.concatenate([input1, input2], axis=1)</span>
        <span class="k">return</span> <span class="n">output_</span></div></div>


<div class="viewcode-block" id="interact_image_list"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.interact_image_list">[docs]</a><span class="k">def</span> <span class="nf">interact_image_list</span><span class="p">(</span><span class="n">img_list</span><span class="p">,</span> <span class="n">num_per_page</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># from wbia.viz import viz_helpers as vh</span>
    <span class="kn">import</span> <span class="nn">plottool</span> <span class="k">as</span> <span class="nn">pt</span>

    <span class="n">nRows</span><span class="p">,</span> <span class="n">nCols</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">get_square_row_cols</span><span class="p">(</span><span class="n">num_per_page</span><span class="p">)</span>
    <span class="n">chunked_iter</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ut</span><span class="o">.</span><span class="n">ichunks</span><span class="p">(</span><span class="n">img_list</span><span class="p">,</span> <span class="n">num_per_page</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">img_chunks</span> <span class="ow">in</span> <span class="n">ut</span><span class="o">.</span><span class="n">InteractiveIter</span><span class="p">(</span><span class="n">chunked_iter</span><span class="p">,</span> <span class="n">display_item</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">pnum_</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">make_pnum_nextgen</span><span class="p">(</span><span class="n">nRows</span><span class="p">,</span> <span class="n">nCols</span><span class="p">)</span>
        <span class="n">pt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">fnum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">doclf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">img_chunks</span><span class="p">:</span>
            <span class="n">pt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">pnum</span><span class="o">=</span><span class="n">pnum_</span><span class="p">())</span>
        <span class="c1"># pt.draw_border(pt.gca(), color=vh.get_truth_color(label))</span>
        <span class="c1"># pt.imshow(patch2, pnum=(1, 2, 2))</span>
        <span class="c1"># pt.draw_border(pt.gca(), color=vh.get_truth_color(label))</span>
        <span class="n">pt</span><span class="o">.</span><span class="n">update</span><span class="p">()</span></div>


<div class="viewcode-block" id="testdata_centersurround"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.testdata_centersurround">[docs]</a><span class="k">def</span> <span class="nf">testdata_centersurround</span><span class="p">(</span><span class="n">item_shape</span><span class="p">):</span>
    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">testdata_input_layer</span><span class="p">(</span><span class="n">item_shape</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">testdata_imglist</span><span class="p">(</span><span class="n">item_shape</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span> <span class="o">=</span> <span class="n">CenterSurroundLayer</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>
    <span class="n">inputdata_</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">convert_cv2_images_to_theano_images</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">,</span> <span class="n">inputdata_</span></div>


<div class="viewcode-block" id="CenterSurroundLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.CenterSurroundLayer">[docs]</a><span class="k">class</span> <span class="nc">CenterSurroundLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># self.name = kwargs.pop(&#39;name&#39;, None)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CenterSurroundLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="CenterSurroundLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.CenterSurroundLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="k">if</span> <span class="n">height</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">width</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s1">&#39;input layer to CenterSurroundLayer should ideally have an even width and height.&#39;</span>
            <span class="p">)</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_shape</span></div>

<div class="viewcode-block" id="CenterSurroundLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.CenterSurroundLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_expr</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        CommandLine:</span>
<span class="sd">            python -m wbia_cnn.custom_layers --test-CenterSurroundLayer.get_output_for:0 --show</span>
<span class="sd">            python -m wbia_cnn.custom_layers --test-CenterSurroundLayer.get_output_for:1 --show</span>

<span class="sd">        Example0:</span>
<span class="sd">            &gt;&gt;&gt; # DISABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; import theano</span>
<span class="sd">            &gt;&gt;&gt; #item_shape = (32, 32, 3)</span>
<span class="sd">            &gt;&gt;&gt; item_shape = (41, 41, 3)</span>
<span class="sd">            &gt;&gt;&gt; self, inputdata_ = testdata_centersurround(item_shape)</span>
<span class="sd">            &gt;&gt;&gt; # Test the actual symbolic expression</span>
<span class="sd">            &gt;&gt;&gt; output_T = utils.evaluate_symbolic_layer(self.get_output_for, inputdata_, T.tensor4, T=theano.tensor)</span>
<span class="sd">            &gt;&gt;&gt; output_T = output_T.astype(np.uint8)</span>
<span class="sd">            &gt;&gt;&gt; ut.quit_if_noshow()</span>
<span class="sd">            &gt;&gt;&gt; img_list = utils.convert_theano_images_to_cv2_images(output_T)</span>
<span class="sd">            &gt;&gt;&gt; interact_image_list(img_list, num_per_page=8)</span>

<span class="sd">        Example1:</span>
<span class="sd">            &gt;&gt;&gt; # DISABLE_DOCTEST</span>
<span class="sd">            &gt;&gt;&gt; from wbia_cnn.custom_layers import *  # NOQA</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; #item_shape = (32, 32, 3)</span>
<span class="sd">            &gt;&gt;&gt; item_shape = (41, 41, 3)</span>
<span class="sd">            &gt;&gt;&gt; self, input_expr = testdata_centersurround(item_shape)</span>
<span class="sd">            &gt;&gt;&gt; # Test using just numpy</span>
<span class="sd">            &gt;&gt;&gt; output_np = self.get_output_for(input_expr, T=np)</span>
<span class="sd">            &gt;&gt;&gt; print(&#39;results agree&#39;)</span>
<span class="sd">            &gt;&gt;&gt; ut.quit_if_noshow()</span>
<span class="sd">            &gt;&gt;&gt; img_list = utils.convert_theano_images_to_cv2_images(output_np)</span>
<span class="sd">            &gt;&gt;&gt; interact_image_list(img_list, num_per_page=8)</span>

<span class="sd">        Ignore:</span>
<span class="sd">            from wbia_cnn import draw_net</span>
<span class="sd">            #draw_net.draw_theano_symbolic_expression(result)</span>
<span class="sd">            assert np.all(output_np == output_T)</span>
<span class="sd">            np.stack = np.vstack</span>
<span class="sd">            T = np</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Create a center and surround for each input patch</span>
        <span class="c1"># return input_</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_expr</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">input_shape</span>

        <span class="n">left_h</span> <span class="o">=</span> <span class="n">height</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">left_w</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">right_h</span> <span class="o">=</span> <span class="n">left_h</span> <span class="o">*</span> <span class="mi">3</span>
        <span class="n">right_w</span> <span class="o">=</span> <span class="n">left_w</span> <span class="o">*</span> <span class="mi">3</span>
        <span class="c1"># account for odd patches</span>
        <span class="n">total_h</span> <span class="o">=</span> <span class="n">left_h</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="n">total_w</span> <span class="o">=</span> <span class="n">left_w</span> <span class="o">*</span> <span class="mi">4</span>

        <span class="n">center</span> <span class="o">=</span> <span class="n">input_expr</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">left_h</span><span class="p">:</span><span class="n">right_h</span><span class="p">,</span> <span class="n">left_w</span><span class="p">:</span><span class="n">right_w</span><span class="p">]</span>
        <span class="n">surround</span> <span class="o">=</span> <span class="n">input_expr</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">total_h</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">total_w</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">output_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_shape_for</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">T</span> <span class="ow">is</span> <span class="n">theano</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
            <span class="n">center</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span>
            <span class="n">surround</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;surround&#39;</span>
            <span class="c1"># production theano version</span>
            <span class="n">output_expr</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="o">*</span><span class="n">output_shape</span><span class="p">)</span>
            <span class="n">output_expr</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;center_surround_alloc&#39;</span>
            <span class="n">set_subtensor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">set_subtensor</span><span class="p">)</span>
            <span class="c1"># set_subtensor = functools.partial(T.set_subtensor, inplace=True, tolerate_inplace_aliasing=True)</span>
            <span class="n">output_expr</span> <span class="o">=</span> <span class="n">set_subtensor</span><span class="p">(</span><span class="n">output_expr</span><span class="p">[::</span><span class="mi">2</span><span class="p">],</span> <span class="n">center</span><span class="p">)</span>
            <span class="n">output_expr</span> <span class="o">=</span> <span class="n">set_subtensor</span><span class="p">(</span><span class="n">output_expr</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">surround</span><span class="p">)</span>
            <span class="n">output_expr</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;center_surround_output&#39;</span>
            <span class="c1"># from wbia_cnn import draw_net</span>
            <span class="c1"># draw_net.draw_theano_symbolic_expression(output_expr)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># debugging numpy version</span>
            <span class="n">output_expr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_expr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">output_expr</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">center</span>
            <span class="n">output_expr</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">surround</span>
        <span class="c1"># output_expr = T.concatenate([center, surround], axis=0)</span>
        <span class="k">return</span> <span class="n">output_expr</span></div></div>


<div class="viewcode-block" id="MultiImageSliceLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageSliceLayer">[docs]</a><span class="k">class</span> <span class="nc">MultiImageSliceLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    orig CyclicSliceLayer</span>
<span class="sd">    References:</span>
<span class="sd">        https://github.com/benanne/kaggle-ndsb/blob/master/dihedral.py#L89</span>

<span class="sd">    This layer stacks rotations of 0, 90, 180, and 270 degrees of the input</span>
<span class="sd">    along the batch dimension.</span>
<span class="sd">    If the input has shape (batch_size, num_channels, r, c),</span>
<span class="sd">    then the output will have shape (4 * batch_size, num_channels, r, c).</span>
<span class="sd">    Note that the stacking happens on axis 0, so a reshape to</span>
<span class="sd">    (4, batch_size, num_channels, r, c) will separate the slice axis.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiImageSliceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>

<div class="viewcode-block" id="MultiImageSliceLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageSliceLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span></div>

<div class="viewcode-block" id="MultiImageSliceLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageSliceLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="c1"># array_tf_0(input_),</span>
                <span class="c1"># array_tf_90(input_),</span>
                <span class="c1"># array_tf_180(input_),</span>
                <span class="c1"># array_tf_270(input_),</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="MultiImageRollLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageRollLayer">[docs]</a><span class="k">class</span> <span class="nc">MultiImageRollLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    orig CyclicConvRollLayer</span>


<span class="sd">    This layer turns (n_views * batch_size, num_channels, r, c) into</span>
<span class="sd">    (n_views * batch_size, n_views * num_channels, r, c) by rolling</span>
<span class="sd">    and concatenating feature maps.</span>
<span class="sd">    It also applies the correct inverse transforms to the r and c</span>
<span class="sd">    dimensions to align the feature maps.</span>

<span class="sd">    References:</span>
<span class="sd">        https://github.com/benanne/kaggle-ndsb/blob/master/dihedral.py#L224</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiImageRollLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inv_tf_funcs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># array_tf_0, array_tf_270, array_tf_180, array_tf_90]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_permutation_matrix</span><span class="p">()</span>

<div class="viewcode-block" id="MultiImageRollLayer.compute_permutation_matrix"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageRollLayer.compute_permutation_matrix">[docs]</a>    <span class="k">def</span> <span class="nf">compute_permutation_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">map_identity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">map_rot90</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

        <span class="n">valid_maps</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_map</span> <span class="o">=</span> <span class="n">map_identity</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">valid_maps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_map</span><span class="p">)</span>
            <span class="n">current_map</span> <span class="o">=</span> <span class="n">current_map</span><span class="p">[</span><span class="n">map_rot90</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">perm_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">valid_maps</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiImageRollLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageRollLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span></div>

<div class="viewcode-block" id="MultiImageRollLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.MultiImageRollLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">input_unfolded</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>

        <span class="n">permuted_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">inv_tf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">perm_matrix</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_tf_funcs</span><span class="p">):</span>
            <span class="n">input_permuted</span> <span class="o">=</span> <span class="n">inv_tf</span><span class="p">(</span><span class="n">input_unfolded</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
            <span class="n">permuted_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_permuted</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="n">permuted_inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># concatenate long the channel axis</span></div></div>


<div class="viewcode-block" id="CyclicPoolLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.CyclicPoolLayer">[docs]</a><span class="k">class</span> <span class="nc">CyclicPoolLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility layer that unfolds the viewpoints dimension and pools over it.</span>
<span class="sd">    Note that this only makes sense for dense representations, not for</span>
<span class="sd">    feature maps (because no inverse transforms are applied to align them).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_layer</span><span class="p">,</span> <span class="n">pool_function</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CyclicPoolLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_function</span> <span class="o">=</span> <span class="n">pool_function</span>

<div class="viewcode-block" id="CyclicPoolLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.CyclicPoolLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></div>

<div class="viewcode-block" id="CyclicPoolLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.CyclicPoolLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">unfolded_input</span> <span class="o">=</span> <span class="n">input_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="n">input_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">input_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_function</span><span class="p">(</span><span class="n">unfolded_input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BatchNormLayer2"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.BatchNormLayer2">[docs]</a><span class="k">class</span> <span class="nc">BatchNormLayer2</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormLayer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds a nonlinearity to batch norm layer to reduce number of layers</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># this_class_now = ut.fix_super_reload_error(BatchNormLayer2, self)</span>
        <span class="n">this_class_now</span> <span class="o">=</span> <span class="n">BatchNormLayer2</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">this_class_now</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">incoming</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># super(BatchNormLayer2, self).__init__(incoming, **kwargs)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nonlinearities</span><span class="o">.</span><span class="n">identity</span> <span class="k">if</span> <span class="n">nonlinearity</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nonlinearity</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BatchNormLayer2.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.BatchNormLayer2.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># this_class_now = ut.fix_super_reload_error(BatchNormLayer2, self)</span>
        <span class="n">this_class_now</span> <span class="o">=</span> <span class="n">BatchNormLayer2</span>
        <span class="n">normalized</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">this_class_now</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_output_for</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># normalized = super(BatchNormLayer2, self).get_output_for(input, **kwargs)</span>
        <span class="n">normalized_activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">normalized_activation</span></div></div>


<div class="viewcode-block" id="FlipLayer"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.FlipLayer">[docs]</a><span class="k">class</span> <span class="nc">FlipLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<div class="viewcode-block" id="FlipLayer.get_output_shape_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.FlipLayer.get_output_shape_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_shape_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_shape</span></div>

<div class="viewcode-block" id="FlipLayer.get_output_for"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.FlipLayer.get_output_for">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></div></div>


<div class="viewcode-block" id="load_json_arch_def"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.load_json_arch_def">[docs]</a><span class="k">def</span> <span class="nf">load_json_arch_def</span><span class="p">(</span><span class="n">arch_json_fpath</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    layer_list = layers.get_all_layers(output_layer)</span>

<span class="sd">    from wbia_cnn import net_strs</span>
<span class="sd">    layer_json_list = [net_strs.make_layer_json_dict(layer)</span>
<span class="sd">                       for layer in layer_list]</span>

<span class="sd">    arch_json_fpath = &#39;/media/raid/work/WS_ALL/_ibsdb/_ibeis_cache/nets/injur-shark_10056_224x224x3_auqbfhle/models/arch_injur-shark_o2_d11_c688_acioqbst/saved_sessions/fit_session_2016-08-26T173854+5/fit_arch_info.json&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">wbia_cnn</span> <span class="kn">import</span> <span class="n">custom_layers</span>
    <span class="kn">import</span> <span class="nn">layers.dnn</span>

    <span class="c1"># FIXME: Need to redo the saved arch json file.</span>
    <span class="c1"># Need to give layers identifiers and specify their inputs / outputs</span>
    <span class="c1"># They are not implicit</span>
    <span class="n">arch_json</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">load_json</span><span class="p">(</span><span class="n">arch_json_fpath</span><span class="p">)</span>
    <span class="n">layer_json_list</span> <span class="o">=</span> <span class="n">arch_json</span><span class="p">[</span><span class="s1">&#39;layers&#39;</span><span class="p">]</span>

    <span class="n">network_def_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer_json</span> <span class="ow">in</span> <span class="n">layer_json_list</span><span class="p">:</span>
        <span class="n">classname</span> <span class="o">=</span> <span class="n">layer_json</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span>
        <span class="n">classkw</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">delete_dict_keys</span><span class="p">(</span><span class="n">layer_json</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">,</span> <span class="s1">&#39;output_shape&#39;</span><span class="p">])</span>

        <span class="c1"># Rectify nonlinearity definition</span>
        <span class="k">if</span> <span class="s1">&#39;nonlinearity&#39;</span> <span class="ow">in</span> <span class="n">classkw</span><span class="p">:</span>
            <span class="n">nonlin</span> <span class="o">=</span> <span class="n">classkw</span><span class="p">[</span><span class="s1">&#39;nonlinearity&#39;</span><span class="p">]</span>
            <span class="n">nonlinclass</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">lasagne</span><span class="o">.</span><span class="n">nonlinearities</span><span class="p">,</span> <span class="n">nonlin</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">],</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">nonlinkw</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">delete_dict_keys</span><span class="p">(</span><span class="n">nonlin</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">nonlin</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">]:</span>
                <span class="n">classkw</span><span class="p">[</span><span class="s1">&#39;nonlinearity&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nonlinclass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">classkw</span><span class="p">[</span><span class="s1">&#39;nonlinearity&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nonlinclass</span><span class="p">(</span><span class="o">**</span><span class="n">nonlinkw</span><span class="p">)</span>

        <span class="n">classtype</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">classtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">classtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="n">classname</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">classtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">classtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">dnn</span><span class="p">,</span> <span class="n">classname</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">classtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">classtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">custom_layers</span><span class="p">,</span> <span class="n">classname</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">classtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layer_func</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">NamedPartial</span><span class="p">(</span><span class="n">classtype</span><span class="p">,</span> <span class="o">**</span><span class="n">classkw</span><span class="p">)</span>
            <span class="n">network_def_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_func</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">network_def_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

    <span class="n">layer_list</span> <span class="o">=</span> <span class="n">custom_layers</span><span class="o">.</span><span class="n">evaluate_layer_list</span><span class="p">(</span><span class="n">network_def_list</span><span class="p">)</span>
    <span class="c1"># Hack, remove all biases before batch norm</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layer_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">BatchNormLayer</span><span class="p">):</span>
            <span class="n">in_layer</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">input_layer</span>
            <span class="k">if</span> <span class="n">in_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">in_layer</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">in_layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">in_layer</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>
                    <span class="n">in_layer</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">output_layer</span> <span class="o">=</span> <span class="n">layer_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">output_layer</span></div>


<div class="viewcode-block" id="evaluate_layer_list"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.evaluate_layer_list">[docs]</a><span class="k">def</span> <span class="nf">evaluate_layer_list</span><span class="p">(</span><span class="n">network_layers_def</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    compiles a sequence of partial functions into a network</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">verbose</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">VERBOSE_CNN</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">network_layers_def</span><span class="p">)</span>
    <span class="n">network_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Evaluting List of </span><span class="si">%d</span><span class="s1"> Layers&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">total</span><span class="p">,))</span>
    <span class="n">layer_fn_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">network_layers_def</span><span class="p">)</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ut</span><span class="o">.</span><span class="n">Indenter</span><span class="p">(</span><span class="s1">&#39; &#39;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="n">verbose</span><span class="p">):</span>
            <span class="n">next_args</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">count</span><span class="p">,</span> <span class="n">layer_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_fn_iter</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="s1">&#39;Evaluating layer </span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1"> (</span><span class="si">%s</span><span class="s1">) &#39;</span>
                        <span class="o">%</span> <span class="p">(</span>
                            <span class="n">count</span><span class="p">,</span>
                            <span class="n">total</span><span class="p">,</span>
                            <span class="n">ut</span><span class="o">.</span><span class="n">get_funcname</span><span class="p">(</span><span class="n">layer_fn</span><span class="p">),</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">with</span> <span class="n">ut</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tt</span><span class="p">:</span>
                    <span class="n">layer</span> <span class="o">=</span> <span class="n">layer_fn</span><span class="p">(</span><span class="o">*</span><span class="n">next_args</span><span class="p">)</span>
                <span class="n">next_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer</span><span class="p">,)</span>
                <span class="n">network_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * took </span><span class="si">%.4f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">toc</span><span class="p">(),))</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="p">,))</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;input_shape&#39;</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.input_shape = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">input_shape</span><span class="p">,))</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.shape = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.output_shape = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">output_shape</span><span class="p">,))</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s1">&#39;layer_fn&#39;</span><span class="p">,</span>
            <span class="s1">&#39;layer_fn.func&#39;</span><span class="p">,</span>
            <span class="s1">&#39;layer_fn.args&#39;</span><span class="p">,</span>
            <span class="s1">&#39;layer_fn.keywords&#39;</span><span class="p">,</span>
            <span class="s1">&#39;layer_fn.__dict__&#39;</span><span class="p">,</span>
            <span class="s1">&#39;layer&#39;</span><span class="p">,</span>
            <span class="s1">&#39;count&#39;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="n">ut</span><span class="o">.</span><span class="n">printex</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Error building layers.</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="s1">&#39;layer=</span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="p">,),</span> <span class="n">keys</span><span class="o">=</span><span class="n">keys</span><span class="p">)</span>
        <span class="k">raise</span>
    <span class="k">return</span> <span class="n">network_layers</span></div>


<span class="c1"># Bundle common layers together</span>


<div class="viewcode-block" id="make_bundles"><a class="viewcode-back" href="../../wbia_cnn.html#wbia_cnn.custom_layers.make_bundles">[docs]</a><span class="k">def</span> <span class="nf">make_bundles</span><span class="p">(</span>
    <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;lru&#39;</span><span class="p">,</span>
    <span class="n">batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">filter_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">pool_stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">branches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>

    <span class="c1"># FIXME; dropout is a pre-operation</span>
    <span class="kn">import</span> <span class="nn">Lasagne</span> <span class="k">as</span> <span class="nn">lasagne</span>
    <span class="kn">import</span> <span class="nn">itertools</span>
    <span class="kn">import</span> <span class="nn">six</span>

    <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># W = init.GlorotUniform()</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">Orthogonal</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>

    <span class="c1"># Rectify default inputs</span>
    <span class="k">if</span> <span class="n">nonlinearity</span> <span class="o">==</span> <span class="s1">&#39;lru&#39;</span><span class="p">:</span>
        <span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearities</span><span class="o">.</span><span class="n">LeakyRectify</span><span class="p">(</span><span class="n">leakiness</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">10.0</span><span class="p">))</span>
    <span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>
    <span class="n">namer</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">next</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">bundles</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">register_bundle</span><span class="p">(</span><span class="n">class_</span><span class="p">):</span>
        <span class="n">classname</span> <span class="o">=</span> <span class="n">ut</span><span class="o">.</span><span class="n">get_classname</span><span class="p">(</span><span class="n">class_</span><span class="p">,</span> <span class="n">local</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">bundles</span><span class="p">[</span><span class="n">classname</span><span class="p">]</span> <span class="o">=</span> <span class="n">class_</span>
        <span class="k">return</span> <span class="n">class_</span>

    <span class="k">class</span> <span class="nc">Bundle</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">suffix</span> <span class="o">=</span> <span class="n">namer</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix</span>

        <span class="k">def</span> <span class="nf">debug_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
            <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;layer = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">layer</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.name = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;input_shape&#39;</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.input_shape = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">input_shape</span><span class="p">,))</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.shape = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">shape</span><span class="p">,))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  * layer.output_shape = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">output_shape</span><span class="p">,))</span>

        <span class="k">def</span> <span class="nf">apply_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
            <span class="c1"># change name standard</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DropoutLayer</span><span class="p">(</span>
                <span class="n">layer</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;D&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">outgoing</span>

        <span class="k">def</span> <span class="nf">apply_batch_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="c1"># change name standard</span>
            <span class="n">nonlinearity</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;nonlinearity&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">nonlinearity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearities</span><span class="o">.</span><span class="n">identity</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">layer</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># bn_name = (kwargs.pop(&#39;name&#39;, None) or</span>
            <span class="c1">#           (getattr(layer, &#39;name&#39;, None) and self.name + &#39;/bn&#39;))</span>
            <span class="n">bn_name</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/bn&#39;</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">BatchNormLayer2</span><span class="p">(</span>
                <span class="n">layer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">bn_name</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="c1"># layer._is_main_layer = False</span>
            <span class="c1"># if nonlinearity is not None:</span>
            <span class="c1">#    nonlin_name = &#39;g&#39; + self.name</span>
            <span class="c1">#    layer = layers.special.NonlinearityLayer(layer,</span>
            <span class="c1">#                                                     nonlinearity,</span>
            <span class="c1">#                                                     name=nonlin_name)</span>
            <span class="c1"># outgoing = layers.normalization.batch_norm(layer, **kwargs)</span>
            <span class="c1"># outgoing.input_layer.name = &#39;bn&#39; + self.name</span>
            <span class="c1"># outgoing.name = &#39;nl&#39; + self.name</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layer</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">InputBundle</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">InputBundle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;I&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GaussianNoiseLayer</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;N&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">ConvBundle</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_filters</span><span class="p">,</span>
            <span class="n">filter_size</span><span class="o">=</span><span class="n">filter_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
            <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">,</span>
            <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">pool_stride</span><span class="o">=</span><span class="n">pool_stride</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">preactivate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span> <span class="o">=</span> <span class="n">num_filters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span> <span class="o">=</span> <span class="n">filter_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">batch_norm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">pool_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span> <span class="o">=</span> <span class="n">pool_stride</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span> <span class="o">=</span> <span class="n">preactivate</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">pool</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">pad</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">super</span><span class="p">(</span><span class="n">ConvBundle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;C&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">suffix</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">incoming</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
                <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">nonlinearity</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">b</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">nonlinearity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">BatchNormLayer2</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/_bn&#39;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">(</span><span class="n">outgoing</span><span class="p">)</span>

            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">Conv2DLayer</span><span class="p">(</span>
                <span class="n">outgoing</span><span class="p">,</span>
                <span class="n">num_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span>
                <span class="n">filter_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">pad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span>
                <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
                <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
                <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">BatchNormLayer2</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/bn_&#39;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">MaxPool2DLayer</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span>
                    <span class="n">pool_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/P&#39;</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">ResidualBundle</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_filters</span><span class="p">,</span>
            <span class="n">filter_size</span><span class="o">=</span><span class="n">filter_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
            <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">pool_stride</span><span class="o">=</span><span class="n">pool_stride</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">preactivate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">postactivate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span> <span class="o">=</span> <span class="n">num_filters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span> <span class="o">=</span> <span class="n">filter_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">pool_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span> <span class="o">=</span> <span class="n">pool_stride</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">pool</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span> <span class="o">=</span> <span class="n">preactivate</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">postactivate</span> <span class="o">=</span> <span class="n">postactivate</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">ResidualBundle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;R&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>

        <span class="c1"># def projectionA(l_inp):</span>
        <span class="c1">#    n_filters = l_inp.output_shape[1] * 2</span>

        <span class="c1">#    def ceildiv(a, b):</span>
        <span class="c1">#        return -(-a // b)</span>

        <span class="c1">#    l = layers.ExpressionLayer(</span>
        <span class="c1">#        l_inp,</span>
        <span class="c1">#        lambda X: X[:, :, ::2, ::2],</span>
        <span class="c1">#        lambda s: (s[0], s[1], ceildiv(s[2], 2), ceildiv(s[3], 2)))</span>
        <span class="c1">#    l = layers.PadLayer(l, [n_filters // 4, 0, 0], batch_ndim=1)</span>
        <span class="c1">#    return l</span>

        <span class="k">def</span> <span class="nf">projectionB</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            The projection shortcut in Eqn.(2) is used to match dimensions</span>
<span class="sd">            (done by 1x1 convolutions). When the shortcuts go across feature</span>
<span class="sd">            maps of two sizes, they are performed with a stride of 2.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># Projection is a strided 1x1 convolution.</span>
            <span class="c1"># I think preactivation should not trigger any nonlinearities. Just</span>
            <span class="c1"># batch normalization. But I haven&#39;t been able to confirm.</span>
            <span class="n">projector</span> <span class="o">=</span> <span class="n">ConvBundle</span><span class="p">(</span>
                <span class="n">filter_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">num_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                <span class="n">pad</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/proj&#39;</span><span class="p">,</span>
                <span class="n">nonlinearity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">preactivate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">shortcut</span> <span class="o">=</span> <span class="n">projector</span><span class="p">(</span><span class="n">incoming</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">shortcut</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            https://github.com/Lasagne/Lasagne/issues/531</span>
<span class="sd">            https://github.com/alrojo/lasagne_residual_network/search?utf8=%E2%9C%93&amp;q=residual</span>
<span class="sd">            https://github.com/FlorianMuellerklein/Identity-Mapping-ResNet-Lasagne/blob/master/models.py</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># Check if this bundle is going to reduce the spatial dimensions</span>
            <span class="n">size_reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span>

            <span class="c1"># Define convolvers</span>
            <span class="n">convkw</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                <span class="n">pad</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">filter_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">filter_size</span><span class="p">,</span>
                <span class="n">num_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_filters</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Do not preactivate if this is the first layer in the network</span>
            <span class="n">convolver1</span> <span class="o">=</span> <span class="n">ConvBundle</span><span class="p">(</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">preactivate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">preactivate</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/C1&#39;</span><span class="p">,</span>
                <span class="o">**</span><span class="n">convkw</span>
            <span class="p">)</span>
            <span class="n">convolver2</span> <span class="o">=</span> <span class="n">ConvBundle</span><span class="p">(</span>
                <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">preactivate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/C2&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">convkw</span>
            <span class="p">)</span>

            <span class="n">branch</span> <span class="o">=</span> <span class="n">incoming</span>
            <span class="n">branch</span> <span class="o">=</span> <span class="n">convolver1</span><span class="p">(</span><span class="n">branch</span><span class="p">)</span>
            <span class="n">branch</span> <span class="o">=</span> <span class="n">convolver2</span><span class="p">(</span><span class="n">branch</span><span class="p">)</span>
            <span class="n">branch</span><span class="o">.</span><span class="n">_is_main_layer</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># Need to project the shortcut branch</span>
            <span class="k">if</span> <span class="n">size_reduced</span><span class="p">:</span>
                <span class="n">shortcut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projectionB</span><span class="p">(</span><span class="n">incoming</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shortcut</span> <span class="o">=</span> <span class="n">incoming</span>

            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">ElemwiseSumLayer</span><span class="p">(</span>
                <span class="p">[</span><span class="n">branch</span><span class="p">,</span> <span class="n">shortcut</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/sum&#39;</span>
            <span class="p">)</span>

            <span class="c1"># Postactivate if this is the last residual layer.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">postactivate</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">BatchNormLayer2</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/bn_&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">MaxPool2DLayer</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span>
                    <span class="n">pool_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/P&#39;</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">InceptionBundle</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="c1"># https://github.com/317070/lasagne-googlenet/blob/master/googlenet.py</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">branches</span><span class="o">=</span><span class="n">branches</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
            <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span>
            <span class="n">pool_stride</span><span class="o">=</span><span class="n">pool_stride</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="c1"># standard</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="o">=</span> <span class="n">branches</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">batch_norm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">pool</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">pool_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span> <span class="o">=</span> <span class="n">pool_stride</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">InceptionBundle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;INCEP&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="n">in_</span> <span class="o">=</span> <span class="n">incoming</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">in_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">(</span><span class="n">in_</span><span class="p">)</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>

            <span class="c1"># branches = self.inception_v0(in_)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">branches</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span>
                        <span class="n">branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span>
                            <span class="n">in_</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;p&#39;</span><span class="p">:</span>
                        <span class="n">branch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b = </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">b</span><span class="p">,))</span>
                        <span class="k">assert</span> <span class="kc">False</span>
                    <span class="n">branches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">branch</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># branches = self.inception_v3_A(in_)</span>
                <span class="n">branches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inception_v0</span><span class="p">(</span><span class="n">in_</span><span class="p">)</span>

            <span class="c1"># print(branches)</span>
            <span class="c1"># for b in branches:</span>
            <span class="c1">#    print(b.output_shape)</span>

            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">ConcatLayer</span><span class="p">(</span><span class="n">branches</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/cat&#39;</span><span class="p">)</span>
            <span class="n">outgoing</span><span class="o">.</span><span class="n">_is_main_layer</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="n">MaxPool2DLayer</span><span class="p">(</span>
                    <span class="n">outgoing</span><span class="p">,</span>
                    <span class="n">pool_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;P&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">outgoing</span>

        <span class="k">def</span> <span class="nf">conv_branch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">,</span> <span class="n">num_reduce</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
            <span class="n">name_aug</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">filter_size</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">num_reduce</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
                    <span class="n">bias</span> <span class="o">=</span> <span class="mf">0.1</span>
                    <span class="n">redu</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">NINLayer</span><span class="p">(</span>
                        <span class="n">incoming</span><span class="p">,</span>
                        <span class="n">num_units</span><span class="o">=</span><span class="n">num_reduce</span><span class="p">,</span>
                        <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                        <span class="n">b</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">bias</span><span class="p">),</span>
                        <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span> <span class="o">+</span> <span class="n">name_aug</span> <span class="o">+</span> <span class="s1">&#39;_reduce&#39;</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">redu</span> <span class="o">=</span> <span class="n">Conv2DLayer</span><span class="p">(</span>
                        <span class="n">incoming</span><span class="p">,</span>
                        <span class="n">num_filters</span><span class="o">=</span><span class="n">num_reduce</span><span class="p">,</span>
                        <span class="n">filter_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                        <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
                        <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span> <span class="o">+</span> <span class="n">name_aug</span> <span class="o">+</span> <span class="s1">&#39;_reduce&#39;</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
                    <span class="n">redu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_batch_norm</span><span class="p">(</span><span class="n">redu</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">redu</span> <span class="o">=</span> <span class="n">incoming</span>
            <span class="n">conv</span> <span class="o">=</span> <span class="n">redu</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
                <span class="n">pad</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">filter_size</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
                <span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2DLayer</span><span class="p">(</span>
                    <span class="n">conv</span><span class="p">,</span>
                    <span class="n">num_filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span>
                    <span class="n">filter_size</span><span class="o">=</span><span class="n">filter_size</span><span class="p">,</span>
                    <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
                    <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span> <span class="o">+</span> <span class="n">name_aug</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">d</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">conv</span><span class="o">.</span><span class="n">_is_main_layer</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
                    <span class="n">conv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_batch_norm</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>
            <span class="c1"># if num_reduce &gt; 0:</span>
            <span class="c1">#    conv._is_main_layer = False</span>
            <span class="k">return</span> <span class="n">conv</span>

        <span class="k">def</span> <span class="nf">proj_branch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">,</span> <span class="n">pool_size</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
            <span class="n">flipped</span> <span class="o">=</span> <span class="n">FlipLayer</span><span class="p">(</span><span class="n">incoming</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/Flip&#39;</span><span class="p">)</span>
            <span class="n">pool</span> <span class="o">=</span> <span class="n">MaxPool2DLayer</span><span class="p">(</span>
                <span class="n">flipped</span><span class="p">,</span>
                <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/pool&#39;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">unflipped</span> <span class="o">=</span> <span class="n">FlipLayer</span><span class="p">(</span><span class="n">pool</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/Unflip&#39;</span><span class="p">)</span>

            <span class="n">project</span> <span class="o">=</span> <span class="n">Conv2DLayer</span><span class="p">(</span>
                <span class="n">unflipped</span><span class="p">,</span>
                <span class="n">num_filters</span><span class="o">=</span><span class="n">num_proj</span><span class="p">,</span>
                <span class="n">filter_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/proj&#39;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
                <span class="n">project</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_batch_norm</span><span class="p">(</span><span class="n">project</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">project</span>

        <span class="k">def</span> <span class="nf">inception_v0</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_</span><span class="p">):</span>
            <span class="c1"># Define the 3 convolutional branches</span>
            <span class="n">conv_branches</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
            <span class="p">]</span>
            <span class="c1"># Define the projection branch</span>
            <span class="n">proj_branches</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">32</span><span class="p">)]</span>
            <span class="n">branches</span> <span class="o">=</span> <span class="n">conv_branches</span> <span class="o">+</span> <span class="n">proj_branches</span>
            <span class="k">return</span> <span class="n">branches</span>

        <span class="k">def</span> <span class="nf">inception_v3_A</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_</span><span class="p">):</span>
            <span class="n">conv_branches</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="p">]</span>
            <span class="n">proj_branches</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_branch</span><span class="p">(</span><span class="n">in_</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">32</span><span class="p">)]</span>
            <span class="n">branches</span> <span class="o">=</span> <span class="n">conv_branches</span> <span class="o">+</span> <span class="n">proj_branches</span>
            <span class="k">return</span> <span class="n">branches</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">DenseBundle</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_units</span><span class="p">,</span>
            <span class="n">batch_norm</span><span class="o">=</span><span class="n">batch_norm</span><span class="p">,</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span> <span class="o">=</span> <span class="n">num_units</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">batch_norm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">nonlinearity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">DenseBundle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">incoming</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">(</span><span class="n">outgoing</span><span class="p">)</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span>
                <span class="n">outgoing</span><span class="p">,</span>
                <span class="n">num_units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;F&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">nonlinearity</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">,</span>
                <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_batch_norm</span><span class="p">(</span><span class="n">outgoing</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">SoftmaxBundle</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span> <span class="o">=</span> <span class="n">num_units</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">batch_norm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">SoftmaxBundle</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">incoming</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">outgoing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_dropout</span><span class="p">(</span><span class="n">outgoing</span><span class="p">)</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span>
                <span class="n">outgoing</span><span class="p">,</span>
                <span class="n">num_units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;F&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">W</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">NonlinearitySoftmax</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">NonlinearityLayer</span><span class="p">(</span>
                <span class="n">incoming</span><span class="p">,</span>
                <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Softmax&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">GlobalPool</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">incoming</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalPoolLayer</span><span class="p">(</span>
                <span class="n">incoming</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;GP&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">outgoing</span><span class="o">.</span><span class="n">_is_main_layer</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">AveragePool</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">incoming</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">outgoing</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalPoolLayer</span><span class="p">(</span>
                <span class="n">incoming</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;AP&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">outgoing</span><span class="o">.</span><span class="n">_is_main_layer</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span> <span class="n">outgoing</span>

    <span class="nd">@register_bundle</span>
    <span class="k">class</span> <span class="nc">MaxPool2D</span><span class="p">(</span><span class="n">Bundle</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> <span class="n">pool_stride</span><span class="o">=</span><span class="n">pool_stride</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">pool_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span> <span class="o">=</span> <span class="n">pool_stride</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">incoming</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">MaxPool2DLayer</span><span class="p">(</span>
                <span class="n">incoming</span><span class="p">,</span>
                <span class="n">pool_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_stride</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;P&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="c1"># def inception_module(l_in, num_1x1, reduce_3x3, num_3x3, reduce_5x5,</span>
    <span class="c1">#                     num_5x5, gain=1.0, bias=0.1):</span>
    <span class="c1">#    &quot;&quot;&quot;</span>
    <span class="c1">#    inception module (without the 3x3x1 pooling and projection because</span>
    <span class="c1">#    that&#39;s difficult in Theano right now)</span>

    <span class="c1">#    http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf</span>
    <span class="c1">#    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bundles</span></div>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CommandLine:</span>
<span class="sd">        python -m wbia_cnn.custom_layers</span>
<span class="sd">        python -m wbia_cnn.custom_layers --allexamples</span>
<span class="sd">        python -m wbia_cnn.custom_layers --allexamples --noface --nosrc</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">multiprocessing</span>

    <span class="n">multiprocessing</span><span class="o">.</span><span class="n">freeze_support</span><span class="p">()</span>  <span class="c1"># for win32</span>
    <span class="kn">import</span> <span class="nn">utool</span> <span class="k">as</span> <span class="nn">ut</span>  <span class="c1"># NOQA</span>

    <span class="n">ut</span><span class="o">.</span><span class="n">doctest_funcs</span><span class="p">()</span>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">wbia-cnn</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../wbia_cnn.html">wbia_cnn package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  <li><a href="../wbia_cnn.html">wbia_cnn</a><ul>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Wild Me.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>